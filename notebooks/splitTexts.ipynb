{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaco/.pyenv/versions/3.9.7/envs/landMarkClassification/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanho do dataset: 2271\n",
      "tamanho do dataset resumido: 393\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "#tokenizerSumLegal = AutoTokenizer.from_pretrained(\"nsi319/legal-led-base-16384\")  \n",
    "#modelSumLegal = AutoModelForSeq2SeqLM.from_pretrained(\"nsi319/legal-led-base-16384\")\n",
    "#summarizerFacebook = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "#tokenizerFaceBook = summarizerFacebook.tokenizer\n",
    "\n",
    "################### LAWMARKCASES ##################################\n",
    "FILE_LAWMARK = '/home/jaco/Projetos/landMarkClassification/data/scraper/scrapee.csv'\n",
    "\n",
    "dfLawMark = pd.read_csv(FILE_LAWMARK)\n",
    "\n",
    "dfLawMark['text'] = dfLawMark['text'].astype(str)\n",
    "dfLawMark['subject'] = dfLawMark['subject'].astype(str)\n",
    "\n",
    "#tirar numeros? fazer uma limpeza? Tirar os 300 nomes de advogados q tem? Tirar os nomes das leis?\n",
    "#tirar os numeros q marcam os paragrafos? Syllabus e Opinion ja resolvem?\n",
    "\n",
    "def limpeza_basica(texto):\n",
    "\n",
    "    texto = texto['text']\n",
    "\n",
    "    texto = texto[169:-124] ### tirando uns html que nao sairam\n",
    "\n",
    "    texto = re.sub('\\. [0-9]+ ', ' ', texto)\n",
    "    texto = re.sub('\\* \\* \\*', ' ', texto)\n",
    "    texto = re.sub('\\.\\' [0-9]+ ', ' ', texto)\n",
    "    texto = re.sub('\\.+ \\.+ \\.+', ' ', texto)\n",
    "    texto = re.sub('\\[.+?omitted\\]', '', texto)\n",
    "    texto = re.sub('\"\\.\" [0-9]+ ', ' ', texto)\n",
    "    texto = re.sub('\\' [0-9]+ [a-z]', ' ', texto,flags=re.IGNORECASE) #deixar todos com essa parada de ignore case?\n",
    "\n",
    "    texto = re.sub('\"\\.\" [0-9]+ ', ' ', texto)\n",
    "\n",
    "    texto = re.sub('\\.[0-9]+ ', '.', texto) #pode mexer muito, muito especifico\n",
    "    texto = re.sub('\\. [0-9]+ ', ' ', texto) #pode mexer muito, muito especifico\n",
    "\n",
    "    texto = re.sub(' +',' ',texto)\n",
    "\n",
    "    return texto\n",
    "\n",
    "dfLawMark['text'] = dfLawMark.apply(lambda row : limpeza_basica(row), axis=1)\n",
    "\n",
    "#dfLawMark.to_csv('/home/jaco/Projetos/landMarkClassification/data/checkAplicacaoRegex.csv',index=False)\n",
    "\n",
    "dfLawMark = dfLawMark[['text','subject']]\n",
    "\n",
    "dfLawMark = dfLawMark[(dfLawMark['text'] != '')] ### checar depois pq aparece isso, o scrape pode retornar nulo? \n",
    "dfLawMark = dfLawMark[dfLawMark[\"text\"].apply(lambda x: len(x) > 1000)] ### checar depois pq aparece isso, pq tem processos que vem do scraper que nao puxa quase nada?\n",
    "dfLawMark = dfLawMark.sample(frac=1)\n",
    "\n",
    "print(f'tamanho do dataset: {len(dfLawMark)}')\n",
    "\n",
    "##testando classificações mais facieis:\n",
    "## tirando classes sem samples suficientes:\n",
    "min_sample_size = 14\n",
    "df2 = dfLawMark.groupby(['subject'],as_index=False).count()\n",
    "df2 = df2[df2['text'] >= min_sample_size]\n",
    "lista = list(df2['subject'])\n",
    "\n",
    "#####\n",
    "#dfLawMark = dfLawMark[dfLawMark['subject'].isin(lista)]\n",
    "dfLawMark = dfLawMark[dfLawMark['subject'].isin(['First Amendment','Freedom of Speech','Criminal Law','Racial Discrimination'])]\n",
    "#####\n",
    "\n",
    "DATSET_SIZE = len(dfLawMark)\n",
    "ROW_COUNTER = 0\n",
    "\n",
    "print(f'tamanho do dataset resumido: {len(dfLawMark)}')\n",
    "\n",
    "### checar se o webscrape ta perfeito vs o site.\n",
    "### pegar um sumarizador profissional law:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>491 U.S 109 S.Ct 105 L.Ed.2d 342 TEXAS, Petiti...</td>\n",
       "      <td>Freedom of Speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>19 U.S 5 L.Ed 6 Wheat COHENSv.VIRGINIA. March ...</td>\n",
       "      <td>Criminal Law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>456 U.S 102 S.Ct 71 L.Ed.2d 732 Carl W. BROWN,...</td>\n",
       "      <td>Freedom of Speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>457 U.S 102 S.Ct 73 L.Ed.2d 248 GLOBE NEWSPAPE...</td>\n",
       "      <td>First Amendment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>283 U.S 51 S.Ct 75 L.Ed NEARv.STATE OF MINNESO...</td>\n",
       "      <td>Freedom of Speech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text            subject\n",
       "1180  491 U.S 109 S.Ct 105 L.Ed.2d 342 TEXAS, Petiti...  Freedom of Speech\n",
       "229   19 U.S 5 L.Ed 6 Wheat COHENSv.VIRGINIA. March ...       Criminal Law\n",
       "1161  456 U.S 102 S.Ct 71 L.Ed.2d 732 Carl W. BROWN,...  Freedom of Speech\n",
       "860   457 U.S 102 S.Ct 73 L.Ed.2d 248 GLOBE NEWSPAPE...    First Amendment\n",
       "1085  283 U.S 51 S.Ct 75 L.Ed NEARv.STATE OF MINNESO...  Freedom of Speech"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfLawMark.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text,char_len):\n",
    "\n",
    "    text = text['text']\n",
    "    \n",
    "    arr = []\n",
    "    tmp_arr = []\n",
    "    char_counter = 0\n",
    "\n",
    "    for n in text:\n",
    "\n",
    "        char_counter = char_counter + 1\n",
    "\n",
    "        tmp_arr.append(n)\n",
    "\n",
    "        if char_counter >= char_len:\n",
    "            arr.append([''.join(tmp_arr)])\n",
    "            char_counter = 0\n",
    "            tmp_arr = []\n",
    "\n",
    "    return arr\n",
    "\n",
    "def ajuste_imbecil(text):\n",
    "    text = text['split']\n",
    "    return text[0]\n",
    "\n",
    "char_num = 2000\n",
    "\n",
    "dfLawMark['split'] = dfLawMark.apply(lambda row : split_text(row,char_num), axis=1)\n",
    "dfLawMark = dfLawMark.explode('split')\n",
    "dfLawMark['split'] = dfLawMark.apply(lambda row : ajuste_imbecil(row), axis=1)\n",
    "dfLawMark = dfLawMark[dfLawMark[\"text\"].apply(lambda x: len(x) > char_num*0.95)]\n",
    "dfLawMark['Syllabus'] = dfLawMark['split']\n",
    "dfLawMark.drop(['split'],inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2912/2613381184.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfEnCode.drop_duplicates(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "encodedClass = pd.factorize(dfLawMark['subject'])\n",
    "dfLawMark['encoded'] = encodedClass[0]+1 ## pytorch precisa começar do 1\n",
    "\n",
    "dfEnCode = dfLawMark[['subject','encoded']]\n",
    "dfEnCode.drop_duplicates(inplace=True)\n",
    "\n",
    "dfEnCode.to_csv('/home/jaco/Projetos/landMarkClassification/data/enCode_Split.csv',index=False)\n",
    "dfLawMark.to_csv('/home/jaco/Projetos/landMarkClassification/data/onlyLandMarkWSyllabus_Split.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15458"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfLawMark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanho do train dataset: 12366\n",
      "tamanho do test dataset: 3092\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dfLawMark['Syllabus']\n",
    "y = dfLawMark['encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=666, stratify=y)\n",
    "\n",
    "dfLawMark_Train = pd.DataFrame()\n",
    "dfLawMark_Train['Syllabus'] = X_train.to_frame()\n",
    "dfLawMark_Train['encoded'] = y_train.to_frame()\n",
    "\n",
    "dfLawMark_Test = pd.DataFrame()\n",
    "dfLawMark_Test['Syllabus'] = X_test.to_frame()\n",
    "dfLawMark_Test['encoded'] = y_test.to_frame()\n",
    "\n",
    "dfLawMark_Train.to_csv('/home/jaco/Projetos/landMarkClassification/data/onlyLandMarkWSyllabus_Train_Split.csv',index=False)\n",
    "print(f'tamanho do train dataset: {len(dfLawMark_Train)}')\n",
    "dfLawMark_Test.to_csv('/home/jaco/Projetos/landMarkClassification/data/onlyLandMarkWSyllabus_Test_Split.csv',index=False)\n",
    "print(f'tamanho do test dataset: {len(dfLawMark_Test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2912/3826867807.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfEnCodeBBC.drop_duplicates(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "mediumBBC = pd.read_csv('/home/jaco/Projetos/landMarkClassification/data/bbc-text.csv')\n",
    "mediumBBC.rename(columns={'category':'subject'},inplace=True)\n",
    "mediumBBC.rename(columns={'text':'Syllabus'},inplace=True)\n",
    "\n",
    "encodedClass = pd.factorize(mediumBBC['subject'])\n",
    "mediumBBC['encoded'] = encodedClass[0]+1 ## pytorch precisa começar do 1\n",
    "\n",
    "dfEnCodeBBC = mediumBBC[['subject','encoded']]\n",
    "dfEnCodeBBC.drop_duplicates(inplace=True)\n",
    "\n",
    "dfEnCodeBBC.to_csv('/home/jaco/Projetos/landMarkClassification/data/enCode_mediumBBC.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanho do train dataset: 1780\n",
      "tamanho do test dataset: 445\n"
     ]
    }
   ],
   "source": [
    "X = mediumBBC['Syllabus']\n",
    "y = mediumBBC['encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=666, stratify=y)\n",
    "\n",
    "mediumBBC_Train = pd.DataFrame()\n",
    "mediumBBC_Train['Syllabus'] = X_train.to_frame()\n",
    "mediumBBC_Train['encoded'] = y_train.to_frame()\n",
    "\n",
    "mediumBBC_Test = pd.DataFrame()\n",
    "mediumBBC_Test['Syllabus'] = X_test.to_frame()\n",
    "mediumBBC_Test['encoded'] = y_test.to_frame()\n",
    "\n",
    "mediumBBC_Train.to_csv('/home/jaco/Projetos/landMarkClassification/data/mediumBBC_Train.csv',index=False)\n",
    "print(f'tamanho do train dataset: {len(mediumBBC_Train)}')\n",
    "mediumBBC_Test.to_csv('/home/jaco/Projetos/landMarkClassification/data/mediumBBC_Test.csv',index=False)\n",
    "print(f'tamanho do test dataset: {len(mediumBBC_Test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('landMarkClassification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e80bf6ea5ecc423e77aaec159991e7c9f008826490951eb9c074501ab4baad85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
