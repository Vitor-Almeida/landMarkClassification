{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaco/.pyenv/versions/3.9.7/envs/landMarkClassification/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanho do dataset: 2271\n",
      "tamanho do dataset resumido: 393\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "#tokenizerSumLegal = AutoTokenizer.from_pretrained(\"nsi319/legal-led-base-16384\")  \n",
    "#modelSumLegal = AutoModelForSeq2SeqLM.from_pretrained(\"nsi319/legal-led-base-16384\")\n",
    "#summarizerFacebook = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "#tokenizerFaceBook = summarizerFacebook.tokenizer\n",
    "\n",
    "################### LAWMARKCASES ##################################\n",
    "FILE_LAWMARK = '/home/jaco/Projetos/landMarkClassification/data/scraper/scrapee.csv'\n",
    "\n",
    "dfLawMark = pd.read_csv(FILE_LAWMARK)\n",
    "\n",
    "dfLawMark['text'] = dfLawMark['text'].astype(str)\n",
    "dfLawMark['subject'] = dfLawMark['subject'].astype(str)\n",
    "\n",
    "#tirar numeros? fazer uma limpeza? Tirar os 300 nomes de advogados q tem? Tirar os nomes das leis?\n",
    "#tirar os numeros q marcam os paragrafos? Syllabus e Opinion ja resolvem?\n",
    "\n",
    "def limpeza_basica(texto):\n",
    "\n",
    "    texto = texto['text']\n",
    "\n",
    "    texto = texto[169:-124] ### tirando uns html que nao sairam\n",
    "\n",
    "    texto = re.sub('\\. [0-9]+ ', ' ', texto)\n",
    "    texto = re.sub('\\* \\* \\*', ' ', texto)\n",
    "    texto = re.sub('\\.\\' [0-9]+ ', ' ', texto)\n",
    "    texto = re.sub('\\.+ \\.+ \\.+', ' ', texto)\n",
    "    texto = re.sub('\\[.+?omitted\\]', '', texto)\n",
    "    texto = re.sub('\"\\.\" [0-9]+ ', ' ', texto)\n",
    "    texto = re.sub('\\' [0-9]+ [a-z]', ' ', texto,flags=re.IGNORECASE) #deixar todos com essa parada de ignore case?\n",
    "\n",
    "    texto = re.sub('\"\\.\" [0-9]+ ', ' ', texto)\n",
    "\n",
    "    texto = re.sub('\\.[0-9]+ ', '.', texto) #pode mexer muito, muito especifico\n",
    "    texto = re.sub('\\. [0-9]+ ', ' ', texto) #pode mexer muito, muito especifico\n",
    "\n",
    "    texto = re.sub(' +',' ',texto)\n",
    "\n",
    "    return texto\n",
    "\n",
    "dfLawMark['text'] = dfLawMark.apply(lambda row : limpeza_basica(row), axis=1)\n",
    "\n",
    "#dfLawMark.to_csv('/home/jaco/Projetos/landMarkClassification/data/checkAplicacaoRegex.csv',index=False)\n",
    "\n",
    "dfLawMark = dfLawMark[['text','subject']]\n",
    "\n",
    "dfLawMark = dfLawMark[(dfLawMark['text'] != '')] ### checar depois pq aparece isso, o scrape pode retornar nulo? \n",
    "dfLawMark = dfLawMark[dfLawMark[\"text\"].apply(lambda x: len(x) > 1000)] ### checar depois pq aparece isso, pq tem processos que vem do scraper que nao puxa quase nada?\n",
    "dfLawMark = dfLawMark.sample(frac=1)\n",
    "\n",
    "print(f'tamanho do dataset: {len(dfLawMark)}')\n",
    "\n",
    "##testando classificações mais facieis:\n",
    "## tirando classes sem samples suficientes:\n",
    "min_sample_size = 14\n",
    "df2 = dfLawMark.groupby(['subject'],as_index=False).count()\n",
    "df2 = df2[df2['text'] >= min_sample_size]\n",
    "lista = list(df2['subject'])\n",
    "\n",
    "#####\n",
    "#dfLawMark = dfLawMark[dfLawMark['subject'].isin(lista)]\n",
    "dfLawMark = dfLawMark[dfLawMark['subject'].isin(['First Amendment','Freedom of Speech','Criminal Law','Racial Discrimination'])]\n",
    "#####\n",
    "\n",
    "DATSET_SIZE = len(dfLawMark)\n",
    "ROW_COUNTER = 0\n",
    "\n",
    "print(f'tamanho do dataset resumido: {len(dfLawMark)}')\n",
    "\n",
    "### checar se o webscrape ta perfeito vs o site.\n",
    "### pegar um sumarizador profissional law:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>431 U.S 97 S.Ct 52 L.Ed.2d 675 Hugh CAREY, etc...</td>\n",
       "      <td>Freedom of Speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>135 U.S 10 S.Ct 34 L.Ed CUNNINGHAM, Sheriff,v....</td>\n",
       "      <td>Criminal Law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>317 U.S 63 S.Ct 87 L.Ed 87 L.Ed Ex parte QUIRI...</td>\n",
       "      <td>Criminal Law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>109 U.S 3 S.Ct 27 L.Ed 'THE CIVIL RIGHTS CASES...</td>\n",
       "      <td>Racial Discrimination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834</th>\n",
       "      <td>349 U.S 75 S.Ct 99 L.Ed Oliver BROWN, et al., ...</td>\n",
       "      <td>Racial Discrimination</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text                subject\n",
       "1149  431 U.S 97 S.Ct 52 L.Ed.2d 675 Hugh CAREY, etc...      Freedom of Speech\n",
       "237   135 U.S 10 S.Ct 34 L.Ed CUNNINGHAM, Sheriff,v....           Criminal Law\n",
       "262   317 U.S 63 S.Ct 87 L.Ed 87 L.Ed Ex parte QUIRI...           Criminal Law\n",
       "1817  109 U.S 3 S.Ct 27 L.Ed 'THE CIVIL RIGHTS CASES...  Racial Discrimination\n",
       "1834  349 U.S 75 S.Ct 99 L.Ed Oliver BROWN, et al., ...  Racial Discrimination"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfLawMark.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text,char_len):\n",
    "\n",
    "    text = text['text']\n",
    "    \n",
    "    arr = []\n",
    "    tmp_arr = []\n",
    "    char_counter = 0\n",
    "\n",
    "    for n in text:\n",
    "\n",
    "        char_counter = char_counter + 1\n",
    "\n",
    "        tmp_arr.append(n)\n",
    "\n",
    "        if char_counter >= char_len:\n",
    "            arr.append([''.join(tmp_arr)])\n",
    "            char_counter = 0\n",
    "            tmp_arr = []\n",
    "\n",
    "    return arr\n",
    "\n",
    "def ajuste_imbecil(text):\n",
    "    text = text['split']\n",
    "    return text[0]\n",
    "\n",
    "char_num = 2000\n",
    "\n",
    "dfLawMark['split'] = dfLawMark.apply(lambda row : split_text(row,char_num), axis=1)\n",
    "dfLawMark = dfLawMark.explode('split')\n",
    "dfLawMark['split'] = dfLawMark.apply(lambda row : ajuste_imbecil(row), axis=1)\n",
    "dfLawMark = dfLawMark[dfLawMark[\"text\"].apply(lambda x: len(x) > char_num*0.95)]\n",
    "dfLawMark['Syllabus'] = dfLawMark['split']\n",
    "dfLawMark.drop(['split'],inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28139/2613381184.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfEnCode.drop_duplicates(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "encodedClass = pd.factorize(dfLawMark['subject'])\n",
    "dfLawMark['encoded'] = encodedClass[0]+1 ## pytorch precisa começar do 1\n",
    "\n",
    "dfEnCode = dfLawMark[['subject','encoded']]\n",
    "dfEnCode.drop_duplicates(inplace=True)\n",
    "\n",
    "dfEnCode.to_csv('/home/jaco/Projetos/landMarkClassification/data/enCode_Split.csv',index=False)\n",
    "dfLawMark.to_csv('/home/jaco/Projetos/landMarkClassification/data/onlyLandMarkWSyllabus_Split.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15458"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfLawMark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanho do train dataset: 12366\n",
      "tamanho do test dataset: 3092\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dfLawMark['Syllabus']\n",
    "y = dfLawMark['encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=666, stratify=y)\n",
    "\n",
    "dfLawMark_Train = pd.DataFrame()\n",
    "dfLawMark_Train['Syllabus'] = X_train.to_frame()\n",
    "dfLawMark_Train['encoded'] = y_train.to_frame()\n",
    "\n",
    "dfLawMark_Test = pd.DataFrame()\n",
    "dfLawMark_Test['Syllabus'] = X_test.to_frame()\n",
    "dfLawMark_Test['encoded'] = y_test.to_frame()\n",
    "\n",
    "dfLawMark_Train.to_csv('/home/jaco/Projetos/landMarkClassification/data/onlyLandMarkWSyllabus_Train_Split.csv',index=False)\n",
    "print(f'tamanho do train dataset: {len(dfLawMark_Train)}')\n",
    "dfLawMark_Test.to_csv('/home/jaco/Projetos/landMarkClassification/data/onlyLandMarkWSyllabus_Test_Split.csv',index=False)\n",
    "print(f'tamanho do test dataset: {len(dfLawMark_Test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediumBBC = pd.read_csv('/home/jaco/Projetos/landMarkClassification/data/bbc-text.csv')\n",
    "mediumBBC.rename(columns={'category':'subject'},inplace=True)\n",
    "mediumBBC.rename(columns={'text':'Syllabus'},inplace=True)\n",
    "\n",
    "encodedClass = pd.factorize(dfLawMark['subject'])\n",
    "dfLawMark['encoded'] = encodedClass[0]+1 ## pytorch precisa começar do 1\n",
    "\n",
    "dfEnCode = dfLawMark[['subject','encoded']]\n",
    "dfEnCode.drop_duplicates(inplace=True)\n",
    "\n",
    "dfEnCode.to_csv('/home/jaco/Projetos/landMarkClassification/data/enCode_Split.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mediumBBC['Syllabus']\n",
    "y = mediumBBC['encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=666, stratify=y)\n",
    "\n",
    "mediumBBC_Train = pd.DataFrame()\n",
    "mediumBBC_Train['Syllabus'] = X_train.to_frame()\n",
    "mediumBBC_Train['encoded'] = y_train.to_frame()\n",
    "\n",
    "mediumBBC_Test = pd.DataFrame()\n",
    "mediumBBC_Test['Syllabus'] = X_test.to_frame()\n",
    "mediumBBC_Test['encoded'] = y_test.to_frame()\n",
    "\n",
    "mediumBBC_Train.to_csv('/home/jaco/Projetos/landMarkClassification/data/mediumBBC_Train.csv',index=False)\n",
    "print(f'tamanho do train dataset: {len(mediumBBC_Train)}')\n",
    "mediumBBC_Test.to_csv('/home/jaco/Projetos/landMarkClassification/data/mediumBBC_Test.csv',index=False)\n",
    "print(f'tamanho do test dataset: {len(mediumBBC_Test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('landMarkClassification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e80bf6ea5ecc423e77aaec159991e7c9f008826490951eb9c074501ab4baad85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
